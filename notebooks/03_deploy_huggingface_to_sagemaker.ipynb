{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deploy Hugging Face Model to AWS SageMaker\n",
                "\n",
                "This notebook demonstrates how to deploy a Hugging Face model to AWS SageMaker."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install sagemaker transformers torch boto3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import os\n",
                "import tempfile\n",
                "import shutil\n",
                "import tarfile\n",
                "import sagemaker\n",
                "import boto3\n",
                "from sagemaker import get_execution_role\n",
                "from sagemaker.huggingface import HuggingFaceModel\n",
                "from transformers import AutoTokenizer, AutoModel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "MODEL_ID = \"microsoft/DialoGPT-medium\"  # Change to your desired model\n",
                "INSTANCE_TYPE = \"ml.m5.large\"  # Adjust based on your needs\n",
                "TRANSFORMERS_VERSION = \"4.26.0\"\n",
                "PYTORCH_VERSION = \"1.13.1\"\n",
                "PY_VERSION = \"py39\"\n",
                "\n",
                "print(f\"Model ID: {MODEL_ID}\")\n",
                "print(f\"Instance Type: {INSTANCE_TYPE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get SageMaker role and session\n",
                "try:\n",
                "    role = get_execution_role()\n",
                "    print(f\"SageMaker execution role: {role}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error getting execution role: {e}\")\n",
                "    print(\"Please set SAGEMAKER_ROLE environment variable or ensure you're in a SageMaker instance\")\n",
                "    # If not in SageMaker, use environment variable\n",
                "    role = os.environ.get('SAGEMAKER_ROLE')\n",
                "    if role:\n",
                "        print(f\"Using role from environment: {role}\")\n",
                "    else:\n",
                "        raise ValueError(\"Please set SAGEMAKER_ROLE environment variable\")\n",
                "\n",
                "sagemaker_session = sagemaker.Session()\n",
                "bucket = sagemaker_session.default_bucket()\n",
                "region = sagemaker_session.boto_region_name\n",
                "\n",
                "print(f\"AWS Region: {region}\")\n",
                "print(f\"Default S3 bucket: {bucket}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create inference script\n",
                "inference_code = '''\n",
                "import json\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "\n",
                "def model_fn(model_dir):\n",
                "    \\\"\\\"\\\"Load the model for inference\\\"\\\"\\\"\n",
                "    model = AutoModel.from_pretrained(model_dir)\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_dir, padding_side='left')\n",
                "    if tokenizer.pad_token is None:\n",
                "        tokenizer.pad_token = tokenizer.eos_token\n",
                "    return {\"model\": model, \"tokenizer\": tokenizer}\n",
                "\n",
                "def input_fn(request_body, request_content_type):\n",
                "    \\\"\\\"\\\"Parse input data\\\"\\\"\\\"\n",
                "    if request_content_type == 'application/json':\n",
                "        input_data = json.loads(request_body)\n",
                "        return input_data.get('inputs', input_data)\n",
                "    return request_body\n",
                "\n",
                "def predict_fn(input_data, model_artifacts):\n",
                "    \\\"\\\"\\\"Make predictions\\\"\\\"\\\"\n",
                "    model = model_artifacts[\"model\"]\n",
                "    tokenizer = model_artifacts[\"tokenizer\"]\n",
                "    \n",
                "    if isinstance(input_data, str):\n",
                "        input_data = [input_data]\n",
                "    \n",
                "    inputs = tokenizer(input_data, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "        predictions = outputs.last_hidden_state.mean(dim=1).tolist()\n",
                "    \n",
                "    return predictions\n",
                "\n",
                "def output_fn(prediction, content_type):\n",
                "    \\\"\\\"\\\"Format output\\\"\\\"\\\"\n",
                "    return json.dumps({\"predictions\": prediction})\n",
                "'''\n",
                "\n",
                "print(\"Inference script created!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and prepare model\n",
                "print(\"Loading model and tokenizer...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side='left')\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "model = AutoModel.from_pretrained(MODEL_ID)\n",
                "print(\"Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Package model for SageMaker\n",
                "temp_dir = tempfile.mkdtemp()\n",
                "model_dir = os.path.join(temp_dir, \"model\")\n",
                "os.makedirs(model_dir, exist_ok=True)\n",
                "\n",
                "print(\"Saving model artifacts...\")\n",
                "tokenizer.save_pretrained(model_dir)\n",
                "model.save_pretrained(model_dir)\n",
                "\n",
                "# Save inference script\n",
                "with open(os.path.join(model_dir, \"inference.py\"), 'w') as f:\n",
                "    f.write(inference_code)\n",
                "\n",
                "print(\"Model artifacts prepared!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create tar.gz archive and upload to S3\n",
                "tar_path = os.path.join(temp_dir, \"model.tar.gz\")\n",
                "\n",
                "print(\"Creating model archive...\")\n",
                "with tarfile.open(tar_path, \"w:gz\") as tar:\n",
                "    tar.add(model_dir, arcname=\".\")\n",
                "\n",
                "print(\"Uploading model to S3...\")\n",
                "model_s3_key = f\"model-artifacts/{MODEL_ID.replace('/', '-')}/model.tar.gz\"\n",
                "sagemaker_session.upload_data(\n",
                "    path=tar_path,\n",
                "    bucket=bucket,\n",
                "    key_prefix=f\"model-artifacts/{MODEL_ID.replace('/', '-')}\"\n",
                ")\n",
                "\n",
                "model_s3_path = f\"s3://{bucket}/{model_s3_key}\"\n",
                "print(f\"Model uploaded to: {model_s3_path}\")\n",
                "\n",
                "# Cleanup temporary files\n",
                "shutil.rmtree(temp_dir, ignore_errors=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create and deploy HuggingFace model\n",
                "print(\"Creating SageMaker model...\")\n",
                "huggingface_model = HuggingFaceModel(\n",
                "    model_data=model_s3_path,\n",
                "    role=role,\n",
                "    transformers_version=TRANSFORMERS_VERSION,\n",
                "    pytorch_version=PYTORCH_VERSION,\n",
                "    py_version=PY_VERSION,\n",
                "    entry_point=\"inference.py\"\n",
                ")\n",
                "\n",
                "print(f\"Deploying to SageMaker endpoint (instance: {INSTANCE_TYPE})...\")\n",
                "predictor = huggingface_model.deploy(\n",
                "    initial_instance_count=1,\n",
                "    instance_type=INSTANCE_TYPE\n",
                ")\n",
                "\n",
                "print(f\"\\n✅ Deployment successful!\")\n",
                "print(f\"Endpoint name: {predictor.endpoint_name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the endpoint\n",
                "test_input = \"Hello, how are you?\"\n",
                "print(f\"Testing endpoint with input: '{test_input}'\")\n",
                "\n",
                "response = predictor.predict({\"inputs\": test_input})\n",
                "print(f\"Response: {response}\")\n",
                "\n",
                "print(\"\\n✅ Model deployed and tested successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cleanup function\n",
                "def cleanup_endpoint(endpoint_name=None):\n",
                "    \"\"\"Delete SageMaker endpoint\"\"\"\n",
                "    if endpoint_name is None:\n",
                "        endpoint_name = predictor.endpoint_name\n",
                "    \n",
                "    sagemaker_client = boto3.client('sagemaker')\n",
                "    print(f\"Deleting endpoint: {endpoint_name}\")\n",
                "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
                "    print(f\"Endpoint {endpoint_name} deletion initiated.\")\n",
                "\n",
                "# Uncomment the line below to delete the endpoint\n",
                "# cleanup_endpoint()\n",
                "\n",
                "print(\"\\n⚠️  Remember to delete the endpoint when done to avoid charges:\")\n",
                "print(f\"cleanup_endpoint('{predictor.endpoint_name}')\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}