{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udd2e Inference with AWS SageMaker Endpoint\n",
    "\n",
    "**Model**: `openai/gpt-oss-120b`  \n",
    "**AWS Profile**: `default`\n",
    "\n",
    "This notebook demonstrates how to perform inference on a deployed SageMaker endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udccb Table of Contents\n",
    "\n",
    "1. [Prerequisites & Setup](#1-prerequisites--setup)\n",
    "2. [Connect to Endpoint](#2-connect-to-endpoint)\n",
    "3. [Basic Inference](#3-basic-inference)\n",
    "4. [Advanced Inference Options](#4-advanced-inference-options)\n",
    "5. [Batch Inference](#5-batch-inference)\n",
    "6. [Performance Testing](#6-performance-testing)\n",
    "7. [Endpoint Monitoring](#7-endpoint-monitoring)\n",
    "8. [Cleanup](#8-cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Prerequisites & Setup\n",
    "\n",
    "Install and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: Install required packages\n",
    "!pip install sagemaker boto3 pandas matplotlib tqdm\n",
    "\n",
    "print(\"\\n\u2705 Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2: Import required libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"\u2705 Libraries imported successfully!\")\n",
    "print(f\"\ud83d\udce6 SageMaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Connect to Endpoint\n",
    "\n",
    "Configure AWS credentials and connect to the deployed SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1: Configure AWS Profile\n",
    "\n",
    "# ============================================\n",
    "# \ud83d\udccc CONFIGURATION - EDIT HERE\n",
    "# ============================================\n",
    "\n",
    "AWS_PROFILE = \"default\"\n",
    "\n",
    "# Set the endpoint name from deployment notebook\n",
    "# Option 1: Manually set the endpoint name\n",
    "ENDPOINT_NAME = None  # Set this if you know your endpoint name, e.g., \"openai-gpt-oss-120b-20231204-121500\"\n",
    "\n",
    "# Option 2: Load from saved configuration file\n",
    "CONFIG_FILE = \"endpoint_config.json\"\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "os.environ['AWS_DEFAULT_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "print(f\"\ud83d\udd10 Using AWS Profile: {AWS_PROFILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.2: Verify AWS credentials and create session\n",
    "\n",
    "try:\n",
    "    # Create session with profile\n",
    "    boto_session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "    sts_client = boto_session.client('sts')\n",
    "    \n",
    "    # Get caller identity to verify credentials\n",
    "    identity = sts_client.get_caller_identity()\n",
    "    \n",
    "    print(\"\u2705 AWS credentials verified successfully!\")\n",
    "    print(f\"   \ud83d\udc64 Account ID: {identity['Account']}\")\n",
    "    print(f\"   \ud83c\udd94 User ARN: {identity['Arn']}\")\n",
    "    \n",
    "    # Create SageMaker session\n",
    "    sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "    region = sagemaker_session.boto_region_name\n",
    "    print(f\"   \ud83c\udf0d AWS Region: {region}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error verifying AWS credentials: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.3: Get endpoint name\n",
    "\n",
    "# Load from config file if not manually set\n",
    "if ENDPOINT_NAME is None:\n",
    "    if os.path.exists(CONFIG_FILE):\n",
    "        with open(CONFIG_FILE, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        ENDPOINT_NAME = config.get(\"endpoint_name\")\n",
    "        print(f\"\ud83d\udcc4 Loaded endpoint from config: {ENDPOINT_NAME}\")\n",
    "    else:\n",
    "        # List available endpoints and let user choose\n",
    "        print(\"\u26a0\ufe0f No endpoint name configured. Listing available endpoints...\")\n",
    "        print()\n",
    "        \n",
    "        sagemaker_client = boto_session.client('sagemaker')\n",
    "        response = sagemaker_client.list_endpoints(\n",
    "            SortBy='CreationTime',\n",
    "            SortOrder='Descending',\n",
    "            MaxResults=10,\n",
    "            StatusEquals='InService'\n",
    "        )\n",
    "        \n",
    "        endpoints = response.get('Endpoints', [])\n",
    "        \n",
    "        if endpoints:\n",
    "            print(\"\ud83d\udccb Available endpoints (InService):\")\n",
    "            for i, ep in enumerate(endpoints, 1):\n",
    "                print(f\"   {i}. {ep['EndpointName']} (Created: {ep['CreationTime']})\")\n",
    "            \n",
    "            # Use the most recent one by default\n",
    "            ENDPOINT_NAME = endpoints[0]['EndpointName']\n",
    "            print(f\"\\n\ud83c\udfaf Using most recent endpoint: {ENDPOINT_NAME}\")\n",
    "            print(\"   (Change ENDPOINT_NAME variable above if needed)\")\n",
    "        else:\n",
    "            print(\"\u274c No InService endpoints found!\")\n",
    "            print(\"   Please run the deployment notebook first.\")\n",
    "else:\n",
    "    print(f\"\ud83d\udcdb Using configured endpoint: {ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.4: Create predictor client\n",
    "\n",
    "try:\n",
    "    # Verify endpoint exists and is InService\n",
    "    sagemaker_client = boto_session.client('sagemaker')\n",
    "    endpoint_info = sagemaker_client.describe_endpoint(EndpointName=ENDPOINT_NAME)\n",
    "    \n",
    "    status = endpoint_info['EndpointStatus']\n",
    "    \n",
    "    if status != 'InService':\n",
    "        print(f\"\u26a0\ufe0f Endpoint status: {status}\")\n",
    "        print(\"   Waiting for endpoint to be InService...\")\n",
    "        \n",
    "        # Wait for endpoint to be ready\n",
    "        waiter = sagemaker_client.get_waiter('endpoint_in_service')\n",
    "        waiter.wait(\n",
    "            EndpointName=ENDPOINT_NAME,\n",
    "            WaiterConfig={'Delay': 30, 'MaxAttempts': 60}\n",
    "        )\n",
    "        print(\"   \u2705 Endpoint is now InService!\")\n",
    "    \n",
    "    # Create predictor\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=ENDPOINT_NAME,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        serializer=JSONSerializer(),\n",
    "        deserializer=JSONDeserializer()\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\u2705 Connected to endpoint: {ENDPOINT_NAME}\")\n",
    "    print(f\"   \ud83d\udcca Status: {status}\")\n",
    "    print(f\"   \ud83d\udcc5 Created: {endpoint_info['CreationTime']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error connecting to endpoint: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Basic Inference\n",
    "\n",
    "Send simple requests to the endpoint and get responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1: Simple text generation\n",
    "\n",
    "def generate_text(prompt: str, max_new_tokens: int = 256, **kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate text using the deployed model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text prompt\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        **kwargs: Additional generation parameters\n",
    "        \n",
    "    Returns:\n",
    "        dict: Response from the model\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        **kwargs\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = predictor.predict(payload)\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"latency_seconds\": latency\n",
    "    }\n",
    "\n",
    "\n",
    "# Test with a simple prompt\n",
    "print(\"\ud83e\uddea Testing basic inference...\")\n",
    "print()\n",
    "\n",
    "test_prompt = \"Hello! Can you tell me a short story about a brave robot?\"\n",
    "\n",
    "result = generate_text(test_prompt, max_new_tokens=200)\n",
    "\n",
    "print(f\"\ud83d\udcdd Prompt: {test_prompt}\")\n",
    "print()\n",
    "print(f\"\ud83d\udcac Response:\")\n",
    "\n",
    "if isinstance(result[\"response\"], dict):\n",
    "    generated_text = result[\"response\"].get(\"generated_text\", result[\"response\"])\n",
    "    if isinstance(generated_text, list):\n",
    "        for text in generated_text:\n",
    "            print(f\"   {text}\")\n",
    "    else:\n",
    "        print(f\"   {generated_text}\")\n",
    "else:\n",
    "    print(f\"   {result['response']}\")\n",
    "\n",
    "print()\n",
    "print(f\"\u23f1\ufe0f Latency: {result['latency_seconds']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Multiple prompts\n",
    "\n",
    "prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain machine learning in simple terms.\",\n",
    "    \"Write a haiku about programming.\",\n",
    "    \"What are the benefits of cloud computing?\"\n",
    "]\n",
    "\n",
    "print(\"\ud83e\uddea Testing with multiple prompts...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n\ud83d\udccc Test {i}/{len(prompts)}\")\n",
    "    print(f\"\ud83d\udcdd Prompt: {prompt}\")\n",
    "    print()\n",
    "    \n",
    "    result = generate_text(prompt, max_new_tokens=150)\n",
    "    \n",
    "    if isinstance(result[\"response\"], dict):\n",
    "        generated_text = result[\"response\"].get(\"generated_text\", [result[\"response\"]])\n",
    "        if isinstance(generated_text, list):\n",
    "            print(f\"\ud83d\udcac Response: {generated_text[0]}\")\n",
    "        else:\n",
    "            print(f\"\ud83d\udcac Response: {generated_text}\")\n",
    "    else:\n",
    "        print(f\"\ud83d\udcac Response: {result['response']}\")\n",
    "    \n",
    "    print(f\"\u23f1\ufe0f Latency: {result['latency_seconds']:.2f}s\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "print(\"\\n\u2705 Multiple prompts testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Advanced Inference Options\n",
    "\n",
    "Explore different generation parameters to control the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1: Temperature variations\n",
    "\n",
    "print(\"\ud83c\udf21\ufe0f Testing different temperature settings...\")\n",
    "print(\"   (Higher temperature = more creative/random)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt = \"Complete this story: Once upon a time in a magical forest, there lived a\"\n",
    "\n",
    "temperatures = [0.1, 0.5, 0.9]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n\ud83c\udf21\ufe0f Temperature: {temp}\")\n",
    "    print(f\"\ud83d\udcdd Prompt: {prompt}\")\n",
    "    \n",
    "    result = generate_text(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        temperature=temp,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = result[\"response\"]\n",
    "    if isinstance(response, dict):\n",
    "        text = response.get(\"generated_text\", [str(response)])\n",
    "        if isinstance(text, list):\n",
    "            text = text[0]\n",
    "    else:\n",
    "        text = str(response)\n",
    "    \n",
    "    print(f\"\ud83d\udcac Response: {text}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.2: Top-p (nucleus sampling) variations\n",
    "\n",
    "print(\"\ud83c\udfaf Testing different top_p settings...\")\n",
    "print(\"   (Lower top_p = more focused/deterministic)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt = \"The best advice for learning to code is\"\n",
    "\n",
    "top_p_values = [0.5, 0.9, 0.95]\n",
    "\n",
    "for top_p in top_p_values:\n",
    "    print(f\"\\n\ud83c\udfaf Top-p: {top_p}\")\n",
    "    \n",
    "    result = generate_text(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=top_p,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = result[\"response\"]\n",
    "    if isinstance(response, dict):\n",
    "        text = response.get(\"generated_text\", [str(response)])\n",
    "        if isinstance(text, list):\n",
    "            text = text[0]\n",
    "    else:\n",
    "        text = str(response)\n",
    "    \n",
    "    print(f\"\ud83d\udcac Response: {text}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.3: Custom generation with all parameters\n",
    "\n",
    "def generate_custom(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    do_sample: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text with full control over parameters.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text\n",
    "        max_new_tokens: Max tokens to generate\n",
    "        temperature: Sampling temperature (0.0-2.0)\n",
    "        top_p: Nucleus sampling probability\n",
    "        top_k: Top-k sampling\n",
    "        repetition_penalty: Penalty for repeated tokens\n",
    "        do_sample: Enable sampling (vs greedy decoding)\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "        \"do_sample\": do_sample\n",
    "    }\n",
    "    \n",
    "    response = predictor.predict(payload)\n",
    "    \n",
    "    if isinstance(response, dict):\n",
    "        text = response.get(\"generated_text\", response)\n",
    "        if isinstance(text, list):\n",
    "            return text[0] if text else str(response)\n",
    "        return str(text)\n",
    "    return str(response)\n",
    "\n",
    "\n",
    "# Test with custom parameters\n",
    "print(\"\u2699\ufe0f Custom generation example:\")\n",
    "print()\n",
    "\n",
    "custom_response = generate_custom(\n",
    "    prompt=\"Write a professional email requesting a meeting:\",\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.5,  # More focused\n",
    "    top_p=0.85,\n",
    "    repetition_penalty=1.2  # Reduce repetition\n",
    ")\n",
    "\n",
    "print(custom_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Batch Inference\n",
    "\n",
    "Process multiple prompts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1: Batch processing function\n",
    "\n",
    "def batch_inference(\n",
    "    prompts: List[str],\n",
    "    max_new_tokens: int = 150,\n",
    "    show_progress: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process multiple prompts and return results as DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of input prompts\n",
    "        max_new_tokens: Max tokens per response\n",
    "        show_progress: Show progress bar\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Results with prompts, responses, and latencies\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    iterator = tqdm(prompts, desc=\"Processing\") if show_progress else prompts\n",
    "    \n",
    "    for prompt in iterator:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = predictor.predict({\n",
    "                \"inputs\": prompt,\n",
    "                \"max_new_tokens\": max_new_tokens\n",
    "            })\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            # Extract text from response\n",
    "            if isinstance(response, dict):\n",
    "                text = response.get(\"generated_text\", str(response))\n",
    "                if isinstance(text, list):\n",
    "                    text = text[0] if text else \"\"\n",
    "            else:\n",
    "                text = str(response)\n",
    "            \n",
    "            results.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": text,\n",
    "                \"latency_seconds\": latency,\n",
    "                \"status\": \"success\"\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": None,\n",
    "                \"latency_seconds\": None,\n",
    "                \"status\": f\"error: {str(e)}\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Test batch inference\n",
    "batch_prompts = [\n",
    "    \"What is Python?\",\n",
    "    \"Explain REST APIs.\",\n",
    "    \"What is Docker?\",\n",
    "    \"Define microservices architecture.\",\n",
    "    \"What is CI/CD?\"\n",
    "]\n",
    "\n",
    "print(\"\ud83d\udce6 Running batch inference...\")\n",
    "results_df = batch_inference(batch_prompts)\n",
    "\n",
    "print(\"\\n\ud83d\udcca Results:\")\n",
    "print(results_df[[\"prompt\", \"latency_seconds\", \"status\"]].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.2: Display batch results\n",
    "\n",
    "print(\"\\n\ud83d\udccb Detailed Batch Results:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    print(f\"\\n\ud83d\udccc Prompt {idx + 1}: {row['prompt']}\")\n",
    "    print(f\"\ud83d\udcac Response: {row['response'][:300] if row['response'] else 'N/A'}...\")\n",
    "    print(f\"\u23f1\ufe0f Latency: {row['latency_seconds']:.2f}s\" if row['latency_seconds'] else \"\u23f1\ufe0f Latency: N/A\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Performance Testing\n",
    "\n",
    "Measure endpoint performance with multiple requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.1: Latency test\n",
    "\n",
    "def run_latency_test(\n",
    "    prompt: str,\n",
    "    num_requests: int = 10,\n",
    "    max_new_tokens: int = 100\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Run latency test with multiple requests.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Test prompt\n",
    "        num_requests: Number of requests to make\n",
    "        max_new_tokens: Tokens per request\n",
    "        \n",
    "    Returns:\n",
    "        dict: Latency statistics\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    \n",
    "    print(f\"\ud83e\uddea Running {num_requests} requests...\")\n",
    "    \n",
    "    for i in tqdm(range(num_requests)):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        predictor.predict({\n",
    "            \"inputs\": prompt,\n",
    "            \"max_new_tokens\": max_new_tokens\n",
    "        })\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        latencies.append(latency)\n",
    "    \n",
    "    return {\n",
    "        \"min\": min(latencies),\n",
    "        \"max\": max(latencies),\n",
    "        \"avg\": sum(latencies) / len(latencies),\n",
    "        \"p50\": sorted(latencies)[len(latencies) // 2],\n",
    "        \"p90\": sorted(latencies)[int(len(latencies) * 0.9)],\n",
    "        \"p99\": sorted(latencies)[int(len(latencies) * 0.99)] if len(latencies) >= 100 else max(latencies),\n",
    "        \"all_latencies\": latencies\n",
    "    }\n",
    "\n",
    "\n",
    "# Run latency test\n",
    "test_prompt = \"Explain the concept of cloud computing in one paragraph.\"\n",
    "latency_stats = run_latency_test(test_prompt, num_requests=10)\n",
    "\n",
    "print(\"\\n\ud83d\udcca Latency Statistics:\")\n",
    "print(f\"   \u2b07\ufe0f Min: {latency_stats['min']:.2f}s\")\n",
    "print(f\"   \u2b06\ufe0f Max: {latency_stats['max']:.2f}s\")\n",
    "print(f\"   \ud83d\udcca Avg: {latency_stats['avg']:.2f}s\")\n",
    "print(f\"   \ud83d\udccf P50: {latency_stats['p50']:.2f}s\")\n",
    "print(f\"   \ud83d\udccf P90: {latency_stats['p90']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.2: Visualize latency distribution\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(latency_stats['all_latencies'], bins=10, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(latency_stats['avg'], color='r', linestyle='--', label=f'Mean: {latency_stats[\"avg\"]:.2f}s')\n",
    "plt.xlabel('Latency (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Latency Distribution')\n",
    "plt.legend()\n",
    "\n",
    "# Line plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(latency_stats['all_latencies']) + 1), latency_stats['all_latencies'], 'b-o')\n",
    "plt.axhline(latency_stats['avg'], color='r', linestyle='--', label=f'Mean: {latency_stats[\"avg\"]:.2f}s')\n",
    "plt.xlabel('Request #')\n",
    "plt.ylabel('Latency (seconds)')\n",
    "plt.title('Latency Over Time')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('latency_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Chart saved to: latency_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.3: Token throughput test\n",
    "\n",
    "print(\"\ud83d\udd04 Testing token throughput at different lengths...\")\n",
    "print()\n",
    "\n",
    "token_lengths = [50, 100, 200, 300]\n",
    "throughput_results = []\n",
    "\n",
    "for max_tokens in token_lengths:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = predictor.predict({\n",
    "        \"inputs\": \"Write a detailed explanation of artificial intelligence.\",\n",
    "        \"max_new_tokens\": max_tokens\n",
    "    })\n",
    "    \n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    # Estimate tokens per second\n",
    "    tokens_per_second = max_tokens / latency\n",
    "    \n",
    "    throughput_results.append({\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"latency\": latency,\n",
    "        \"tokens_per_second\": tokens_per_second\n",
    "    })\n",
    "    \n",
    "    print(f\"   \ud83d\udccf {max_tokens} tokens: {latency:.2f}s ({tokens_per_second:.1f} tokens/sec)\")\n",
    "\n",
    "# Create throughput DataFrame\n",
    "throughput_df = pd.DataFrame(throughput_results)\n",
    "print(\"\\n\ud83d\udcca Throughput Summary:\")\n",
    "print(throughput_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Endpoint Monitoring\n",
    "\n",
    "Monitor endpoint metrics and CloudWatch statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.1: Get endpoint details\n",
    "\n",
    "sagemaker_client = boto_session.client('sagemaker')\n",
    "\n",
    "try:\n",
    "    endpoint_response = sagemaker_client.describe_endpoint(EndpointName=ENDPOINT_NAME)\n",
    "    endpoint_config = sagemaker_client.describe_endpoint_config(\n",
    "        EndpointConfigName=endpoint_response['EndpointConfigName']\n",
    "    )\n",
    "    \n",
    "    print(\"\ud83d\udcca Endpoint Details:\")\n",
    "    print(f\"   \ud83d\udcdb Name: {endpoint_response['EndpointName']}\")\n",
    "    print(f\"   \ud83d\udd04 Status: {endpoint_response['EndpointStatus']}\")\n",
    "    print(f\"   \ud83d\udcc5 Created: {endpoint_response['CreationTime']}\")\n",
    "    print(f\"   \ud83d\udd04 Last Modified: {endpoint_response['LastModifiedTime']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"\u2699\ufe0f Configuration:\")\n",
    "    for variant in endpoint_config['ProductionVariants']:\n",
    "        print(f\"   \ud83d\udcbb Instance Type: {variant['InstanceType']}\")\n",
    "        print(f\"   \ud83d\udd22 Instance Count: {variant.get('InitialInstanceCount', 'N/A')}\")\n",
    "        print(f\"   \ud83d\udcca Variant Name: {variant['VariantName']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error getting endpoint details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.2: CloudWatch metrics\n",
    "\n",
    "cloudwatch = boto_session.client('cloudwatch')\n",
    "\n",
    "# Define time range (last 1 hour)\n",
    "end_time = datetime.datetime.utcnow()\n",
    "start_time = end_time - datetime.timedelta(hours=1)\n",
    "\n",
    "metrics = [\n",
    "    ('Invocations', 'Sum'),\n",
    "    ('ModelLatency', 'Average'),\n",
    "    ('OverheadLatency', 'Average'),\n",
    "    ('Invocation4XXErrors', 'Sum'),\n",
    "    ('Invocation5XXErrors', 'Sum')\n",
    "]\n",
    "\n",
    "print(\"\ud83d\udcca CloudWatch Metrics (Last Hour):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for metric_name, stat in metrics:\n",
    "    try:\n",
    "        response = cloudwatch.get_metric_statistics(\n",
    "            Namespace='AWS/SageMaker',\n",
    "            MetricName=metric_name,\n",
    "            Dimensions=[\n",
    "                {'Name': 'EndpointName', 'Value': ENDPOINT_NAME},\n",
    "                {'Name': 'VariantName', 'Value': 'AllTraffic'}\n",
    "            ],\n",
    "            StartTime=start_time,\n",
    "            EndTime=end_time,\n",
    "            Period=300,\n",
    "            Statistics=[stat]\n",
    "        )\n",
    "        \n",
    "        datapoints = response.get('Datapoints', [])\n",
    "        if datapoints:\n",
    "            value = sum(dp[stat] for dp in datapoints)\n",
    "            if 'Latency' in metric_name:\n",
    "                value = value / len(datapoints) / 1000  # Convert to seconds\n",
    "                print(f\"   {metric_name}: {value:.3f}s\")\n",
    "            else:\n",
    "                print(f\"   {metric_name}: {value:.0f}\")\n",
    "        else:\n",
    "            print(f\"   {metric_name}: No data\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   {metric_name}: Error - {e}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Cleanup\n",
    "\n",
    "Delete the endpoint when you're done to avoid unnecessary costs.\n",
    "\n",
    "> **\u26a0\ufe0f Warning**: Only run this when you no longer need the endpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.1: List all endpoints\n",
    "\n",
    "print(\"\ud83d\udccb Active SageMaker Endpoints:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "response = sagemaker_client.list_endpoints(\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=20\n",
    ")\n",
    "\n",
    "for endpoint in response['Endpoints']:\n",
    "    print(f\"   \ud83d\udcdb {endpoint['EndpointName']}\")\n",
    "    print(f\"      Status: {endpoint['EndpointStatus']}\")\n",
    "    print(f\"      Created: {endpoint['CreationTime']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.2: Delete endpoint function\n",
    "\n",
    "def delete_endpoint(endpoint_name: str, force: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Delete a SageMaker endpoint.\n",
    "    \n",
    "    Args:\n",
    "        endpoint_name: Name of the endpoint to delete\n",
    "        force: Skip confirmation prompt\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if deleted successfully\n",
    "    \"\"\"\n",
    "    if not force:\n",
    "        confirm = input(f\"\u26a0\ufe0f Delete endpoint '{endpoint_name}'? (yes/no): \")\n",
    "        if confirm.lower() != 'yes':\n",
    "            print(\"\u274c Deletion cancelled.\")\n",
    "            return False\n",
    "    \n",
    "    try:\n",
    "        # Delete endpoint\n",
    "        sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "        print(f\"\u2705 Endpoint '{endpoint_name}' deletion initiated.\")\n",
    "        \n",
    "        # Get endpoint config name\n",
    "        try:\n",
    "            config_name = f\"{endpoint_name}-config\"\n",
    "            sagemaker_client.delete_endpoint_config(EndpointConfigName=config_name)\n",
    "            print(f\"\u2705 Endpoint config '{config_name}' deleted.\")\n",
    "        except:\n",
    "            pass  # Config might have different name\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error deleting endpoint: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "print(\"\ud83d\uddd1\ufe0f Delete Endpoint Function Ready\")\n",
    "print(f\"   To delete: delete_endpoint('{ENDPOINT_NAME}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u26a0\ufe0f UNCOMMENT THE LINE BELOW TO DELETE THE ENDPOINT\n",
    "# Only run this when you're COMPLETELY done with the endpoint!\n",
    "\n",
    "# delete_endpoint(ENDPOINT_NAME)\n",
    "\n",
    "# Or to skip confirmation:\n",
    "# delete_endpoint(ENDPOINT_NAME, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udcdd Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "1. **Setup**: Installing packages and connecting to AWS\n",
    "2. **Connection**: Connecting to the deployed SageMaker endpoint\n",
    "3. **Basic Inference**: Simple text generation\n",
    "4. **Advanced Options**: Temperature, top-p, and other parameters\n",
    "5. **Batch Processing**: Processing multiple prompts efficiently\n",
    "6. **Performance Testing**: Latency and throughput analysis\n",
    "7. **Monitoring**: CloudWatch metrics and endpoint details\n",
    "8. **Cleanup**: Deleting endpoints to save costs\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# Simple inference\n",
    "response = predictor.predict({\n",
    "    \"inputs\": \"Your prompt here\",\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9\n",
    "})\n",
    "```\n",
    "\n",
    "### Useful Links\n",
    "\n",
    "- [SageMaker Python SDK](https://sagemaker.readthedocs.io/)\n",
    "- [Hugging Face on SageMaker](https://huggingface.co/docs/sagemaker/)\n",
    "- [AWS SageMaker Documentation](https://docs.aws.amazon.com/sagemaker/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}