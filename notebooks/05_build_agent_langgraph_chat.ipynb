{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59337e2e",
   "metadata": {},
   "source": [
    "# SageMaker Chat Integration with LangGraph\n",
    "\n",
    "This notebook demonstrates how to invoke a DeepSeek SageMaker endpoint using LangGraph for stateful chat management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "!pip install -r requirements.txt -q\n",
    "# Install sagemaker separately to avoid strict numpy pinning on Python 3.13\n",
    "!pip install 'sagemaker==2.251.1' --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "from sagemaker.predictor import retrieve_default\n",
    "from typing import Annotated, TypedDict, List, Dict, Any\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Configuration\n",
    "PROFILE_NAME = 'default'\n",
    "ENDPOINT_NAME = \"jumpstart-dft-deepseek-llm-r1-disti-20251206-053241\"\n",
    "\n",
    "try:\n",
    "    REGION_NAME = boto3.Session(profile_name=PROFILE_NAME).region_name\n",
    "except Exception:\n",
    "    REGION_NAME = boto3.Session().region_name\n",
    "\n",
    "print(f\"Using Endpoint: {ENDPOINT_NAME}\")\n",
    "print(f\"Region: {REGION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ebaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Predictor\n",
    "# Create a Boto3 session with the specific profile\n",
    "try:\n",
    "    boto_session = boto3.Session(profile_name=PROFILE_NAME)\n",
    "    # Create a SageMaker session using the boto3 session\n",
    "    sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "    print(f\"Authenticated with profile: {PROFILE_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to use profile {PROFILE_NAME}, falling back to default. Error: {e}\")\n",
    "    sagemaker_session = None\n",
    "\n",
    "# retrieve_default automatically configures the predictor based on the endpoint\n",
    "if sagemaker_session:\n",
    "    predictor = retrieve_default(ENDPOINT_NAME, sagemaker_session=sagemaker_session)\n",
    "else:\n",
    "    predictor = retrieve_default(ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph State\n",
    "class State(TypedDict):\n",
    "    # Messages list stores the conversation history\n",
    "    messages: List[Dict[str, str]]\n",
    "\n",
    "# Define the Chat Node\n",
    "def call_model(state: State):\n",
    "    print(\"Invoking model...\")\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Prepare payload for DeepSeek model\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 512,  # Adjust token limit as needed\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Invoke endpoint\n",
    "        response = predictor.predict(payload)\n",
    "        \n",
    "        # Parse response\n",
    "        if isinstance(response, bytes):\n",
    "            response_data = json.loads(response.decode('utf-8'))\n",
    "        else:\n",
    "            response_data = response\n",
    "            \n",
    "        # print(f\"Raw Response: {response_data}\") # Uncomment for debugging\n",
    "        \n",
    "        # Extract content\n",
    "        content = \"\"\n",
    "        if isinstance(response_data, dict) and 'choices' in response_data:\n",
    "             content = response_data['choices'][0]['message']['content']\n",
    "        elif isinstance(response_data, list) and len(response_data) > 0 and 'generated_text' in response_data[0]:\n",
    "             content = response_data[0]['generated_text']\n",
    "        else:\n",
    "             # Fallback\n",
    "             content = str(response_data)\n",
    "             \n",
    "        # Create assistant message\n",
    "        assistant_message = {\"role\": \"assistant\", \"content\": content}\n",
    "        \n",
    "        # Return updated state\n",
    "        return {\"messages\": messages + [assistant_message]}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking endpoint: {e}\")\n",
    "        return {\"messages\": messages + [{\"role\": \"assistant\", \"content\": \"Error: Failed to get response.\"}]}\n",
    "\n",
    "# Build the Graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"deepseek_agent\", call_model)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"deepseek_agent\")\n",
    "workflow.add_edge(\"deepseek_agent\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Graph with a single turn\n",
    "initial_state = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Create a Flappy Bird game in Python.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "output = app.invoke(initial_state)\n",
    "print(\"\\n--- Final Output ---\")\n",
    "print(output[\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Chat Function\n",
    "def chat_session():\n",
    "    print(\"Starting chat session. Type 'quit' to exit.\")\n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "            break\n",
    "            \n",
    "        conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Run graph\n",
    "        result = app.invoke({\"messages\": conversation_history})\n",
    "        \n",
    "        # Update history with the result\n",
    "        conversation_history = result[\"messages\"]\n",
    "        \n",
    "        print(f\"Assistant: {conversation_history[-1]['content']}\")\n",
    "\n",
    "# chat_session() # Uncomment to run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}