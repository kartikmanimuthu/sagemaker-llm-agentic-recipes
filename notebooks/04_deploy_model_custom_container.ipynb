{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Deploying Sampl Model from Hugging Face to AWS SageMaker\n",
				"\n",
				"This notebook demonstrates how to deploy a Sampl model from Hugging Face to AWS SageMaker and perform inference.\n",
				"\n",
				"## Table of Contents\n",
				"1. [Setup and Installation](#setup-and-installation)\n",
				"2. [AWS Configuration](#aws-configuration)\n",
				"3. [Model Preparation](#model-preparation)\n",
				"4. [SageMaker Deployment](#sagemaker-deployment)\n",
				"5. [Inference Testing](#inference-testing)\n",
				"6. [Monitoring and Cleanup](#monitoring-and-cleanup)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Setup and Installation"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Install required packages\n",
				"!pip install sagemaker transformers torch datasets boto3\n",
				"\n",
				"# Import libraries\n",
				"import sagemaker\n",
				"import boto3\n",
				"import json\n",
				"import time\n",
				"import os\n",
				"from sagemaker import get_execution_role\n",
				"from sagemaker.huggingface import HuggingFaceModel, HuggingFaceProcessor\n",
				"from transformers import AutoTokenizer, AutoModel\n",
				"import torch\n",
				"from datasets import Dataset\n",
				"import pandas as pd\n",
				"\n",
				"print(\"Libraries imported successfully!\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## AWS Configuration"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Configure AWS and SageMaker\n",
				"try:\n",
				"    # Get execution role\n",
				"    role = get_execution_role()\n",
				"    print(f\"SageMaker execution role: {role}\")\n",
				"except Exception as e:\n",
				"    print(f\"Error getting execution role: {e}\")\n",
				"    print(\"Please ensure you're running this notebook in a SageMaker instance or have proper AWS credentials configured\")\n",
				"\n",
				"# Initialize SageMaker session\n",
				"sagemaker_session = sagemaker.Session()\n",
				"region = sagemaker_session.boto_region_name\n",
				"print(f\"AWS Region: {region}\")\n",
				"\n",
				"# Create S3 bucket for model artifacts (if needed)\n",
				"bucket = sagemaker_session.default_bucket()\n",
				"print(f\"Default S3 bucket: {bucket}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Model Preparation"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Define model configuration\n",
				"MODEL_ID = \"microsoft/DialoGPT-medium\"  # You can change this to the specific Sampl model you want\n",
				"MODEL_VERSION = \"1.0.0\"\n",
				"\n",
				"print(f\"Model ID: {MODEL_ID}\")\n",
				"print(f\"Model Version: {MODEL_VERSION}\")\n",
				"\n",
				"# Test model locally first\n",
				"try:\n",
				"    print(\"Loading model locally for testing...\")\n",
				"    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side='left')\n",
				"    if tokenizer.pad_token is None:\n",
				"        tokenizer.pad_token = tokenizer.eos_token\n",
				"    \n",
				"    model = AutoModel.from_pretrained(MODEL_ID)\n",
				"    print(\"Model loaded successfully!\")\n",
				"    print(f\"Model type: {type(model)}\")\n",
				"    print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
				"except Exception as e:\n",
				"    print(f\"Error loading model: {e}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Create inference script\n",
				"inference_code = '''\n",
				"import json\n",
				"import os\n",
				"import torch\n",
				"from transformers import AutoTokenizer, AutoModel\n",
				"import traceback\n",
				"\n",
				"def model_fn(model_dir):\n",
				"    \"\"\"Load the model for inference\"\"\"\n",
				"    try:\n",
				"        print(\"Loading model and tokenizer...\")\n",
				"        model = AutoModel.from_pretrained(model_dir)\n",
				"        tokenizer = AutoTokenizer.from_pretrained(model_dir, padding_side='left')\n",
				"        \n",
				"        if tokenizer.pad_token is None:\n",
				"            tokenizer.pad_token = tokenizer.eos_token\n",
				"            \n",
				"        print(\"Model and tokenizer loaded successfully!\")\n",
				"        return {\"model\": model, \"tokenizer\": tokenizer}\n",
				"    except Exception as e:\n",
				"        print(f\"Error in model_fn: {e}\")\n",
				"        print(traceback.format_exc())\n",
				"        raise\n",
				"\n",
				"def input_fn(request_body, request_content_type):\n",
				"    \"\"\"Parse input data\"\"\"\n",
				"    try:\n",
				"        if request_content_type == 'application/json':\n",
				"            input_data = json.loads(request_body)\n",
				"            if 'inputs' in input_data:\n",
				"                return input_data['inputs']\n",
				"            else:\n",
				"                return input_data\n",
				"        else:\n",
				"            return request_body\n",
				"    except Exception as e:\n",
				"        print(f\"Error in input_fn: {e}\")\n",
				"        return request_body\n",
				"\n",
				"def predict_fn(input_data, model_artifacts):\n",
				"    \"\"\"Make predictions\"\"\"\n",
				"    try:\n",
				"        model = model_artifacts[\"model\"]\n",
				"        tokenizer = model_artifacts[\"tokenizer\"]\n",
				"        \n",
				"        if isinstance(input_data, str):\n",
				"            input_data = [input_data]\n",
				"        \n",
				"        # Tokenize input\n",
				"        inputs = tokenizer(input_data, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
				"        \n",
				"        # Make prediction\n",
				"        with torch.no_grad():\n",
				"            outputs = model(**inputs)\n",
				"            # For this example, we'll return the last hidden states\n",
				"            predictions = outputs.last_hidden_state.mean(dim=1).tolist()\n",
				"        \n",
				"        return predictions\n",
				"    except Exception as e:\n",
				"        print(f\"Error in predict_fn: {e}\")\n",
				"        print(traceback.format_exc())\n",
				"        return {\"error\": str(e)}\n",
				"\n",
				"def output_fn(prediction, content_type):\n",
				"    \"\"\"Format output\"\"\"\n",
				"    try:\n",
				"        if content_type == 'application/json':\n",
				"            return json.dumps({\"predictions\": prediction}, indent=2)\n",
				"        else:\n",
				"            return str(prediction)\n",
				"    except Exception as e:\n",
				"        print(f\"Error in output_fn: {e}\")\n",
				"        return json.dumps({\"error\": str(e)})\n",
				"'''\n",
				"\n",
				"# Save inference code to file\n",
				"with open('/tmp/inference.py', 'w') as f:\n",
				"    f.write(inference_code)\n",
				"\n",
				"print(\"Inference script created successfully!\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## SageMaker Deployment"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Create HuggingFace Model\n",
				"try:\n",
				"    # Create HuggingFace Model\n",
				"    huggingface_model = HuggingFaceModel(\n",
				"        model_data=f\"s3://{bucket}/model-artifacts/{MODEL_ID.replace('/', '-')}/model.tar.gz\",\n",
				"        role=role,\n",
				"        transformers_version=\"4.26.0\",\n",
				"        pytorch_version=\"1.13.1\",\n",
				"        py_version=\"py39\",\n",
				"        entry_point=\"inference.py\",\n",
				"        source_dir=\"/tmp/\"\n",
				"    )\n",
				"    \n",
				"    print(\"HuggingFace Model created successfully!\")\n",
				"    print(f\"Model data: {huggingface_model.model_data}\")\n",
				"    print(f\"Role: {huggingface_model.role}\")\n",
				"    \n",
				"except Exception as e:\n",
				"    print(f\"Error creating HuggingFace model: {e}\")\n",
				"    print(\"This might be because we haven't uploaded the model to S3 yet. Let's do that first.\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Prepare and upload model to S3\n",
				"print(\"Preparing model for S3 upload...\")\n",
				"\n",
				"# Create temporary directory for model files\n",
				"import tempfile\n",
				"import shutil\n",
				"\n",
				"temp_dir = tempfile.mkdtemp()\n",
				"model_dir = os.path.join(temp_dir, \"model\")\n",
				"os.makedirs(model_dir, exist_ok=True)\n",
				"\n",
				"try:\n",
				"    # Save model and tokenizer\n",
				"    print(\"Saving model and tokenizer...\")\n",
				"    tokenizer.save_pretrained(model_dir)\n",
				"    model.save_pretrained(model_dir)\n",
				"    \n",
				"    # Copy inference script to model directory\n",
				"    shutil.copy(\"/tmp/inference.py\", model_dir)\n",
				"    \n",
				"    # Create tar.gz file\n",
				"    import tarfile\n",
				"    tar_path = \"/tmp/model.tar.gz\"\n",
				"    with tarfile.open(tar_path, \"w:gz\") as tar:\n",
				"        tar.add(model_dir, arcname=\".\")\n",
				"    \n",
				"    print(f\"Model packaged successfully at: {tar_path}\")\n",
				"    \n",
				"    # Upload to S3\n",
				"    model_s3_path = f\"s3://{bucket}/model-artifacts/{MODEL_ID.replace('/', '-')}/model.tar.gz\"\n",
				"    print(f\"Uploading model to: {model_s3_path}\")\n",
				"    \n",
				"    sagemaker_session.upload_data(\n",
				"        path=tar_path,\n",
				"        bucket=bucket,\n",
				"        key_prefix=f\"model-artifacts/{MODEL_ID.replace('/', '-')}\"\n",
				"    )\n",
				"    \n",
				"    print(\"Model uploaded to S3 successfully!\")\n",
				"    \n",
				"finally:\n",
				"    # Clean up temporary files\n",
				"    shutil.rmtree(temp_dir, ignore_errors=True)\n",
				"    if os.path.exists(\"/tmp/model.tar.gz\"):\n",
				"        os.remove(\"/tmp/model.tar.gz\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Now create and deploy the model\n",
				"try:\n",
				"    # Create HuggingFace Model with actual S3 path\n",
				"    model_s3_path = f\"s3://{bucket}/model-artifacts/{MODEL_ID.replace('/', '-')}/model.tar.gz\"\n",
				"    \n",
				"    huggingface_model = HuggingFaceModel(\n",
				"        model_data=model_s3_path,\n",
				"        role=role,\n",
				"        transformers_version=\"4.26.0\",\n",
				"        pytorch_version=\"1.13.1\",\n",
				"        py_version=\"py39\",\n",
				"        entry_point=\"inference.py\"\n",
				"    )\n",
				"    \n",
				"    print(\"Model object created successfully!\")\n",
				"    \n",
				"    # Deploy the model\n",
				"    print(\"Deploying model to SageMaker endpoint...\")\n",
				"    predictor = huggingface_model.deploy(\n",
				"        initial_instance_count=1,\n",
				"        instance_type=\"ml.m5.large\"  # You can change this based on your needs\n",
				"    )\n",
				"    \n",
				"    print(\"Model deployed successfully!\")\n",
				"    print(f\"Endpoint name: {predictor.endpoint_name}\")\n",
				"    \n",
				"except Exception as e:\n",
				"    print(f\"Error deploying model: {e}\")\n",
				"    print(traceback.format_exc())"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Inference Testing"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Test the deployed model\n",
				"try:\n",
				"    # Test data\n",
				"    test_inputs = [\n",
				"        \"Hello, how are you today?\",\n",
				"        \"What is the weather like?\",\n",
				"        \"Tell me about machine learning.\"\n",
				"    ]\n",
				"    \n",
				"    print(\"Testing model inference...\")\n",
				"    \n",
				"    for i, test_input in enumerate(test_inputs, 1):\n",
				"        print(f\"\\nTest {i}:\")\n",
				"        print(f\"Input: {test_input}\")\n",
				"        \n",
				"        # Make prediction\n",
				"        response = predictor.predict({\n",
				"            \"inputs\": test_input\n",
				"        })\n",
				"        \n",
				"        print(f\"Response: {response}\")\n",
				"        print(\"-\" * 50)\n",
				"    \n",
				"    print(\"Inference testing completed successfully!\")\n",
				"    \n",
				"except Exception as e:\n",
				"    print(f\"Error during inference testing: {e}\")\n",
				"    print(traceback.format_exc())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Performance testing\n",
				"try:\n",
				"    print(\"Running performance tests...\")\n",
				"    \n",
				"    import time\n",
				"    \n",
				"    # Test with multiple requests\n",
				"    test_inputs = [\"Hello world!\", \"How are you?\", \"What's new?\", \"Tell me something interesting.\"]\n",
				"    \n",
				"    response_times = []\n",
				"    \n",
				"    for i, test_input in enumerate(test_inputs, 1):\n",
				"        start_time = time.time()\n",
				"        \n",
				"        response = predictor.predict({\n",
				"            \"inputs\": test_input\n",
				"        })\n",
				"        \n",
				"        end_time = time.time()\n",
				"        response_time = end_time - start_time\n",
				"        response_times.append(response_time)\n",
				"        \n",
				"        print(f\"Request {i}: {response_time:.2f} seconds\")\n",
				"    \n",
				"    # Calculate statistics\n",
				"    avg_response_time = sum(response_times) / len(response_times)\n",
				"    min_response_time = min(response_times)\n",
				"    max_response_time = max(response_times)\n",
				"    \n",
				"    print(f\"\\nPerformance Summary:\")\n",
				"    print(f\"Average response time: {avg_response_time:.2f} seconds\")\n",
				"    print(f\"Minimum response time: {min_response_time:.2f} seconds\")\n",
				"    print(f\"Maximum response time: {max_response_time:.2f} seconds\")\n",
				"    \n",
				"except Exception as e:\n",
				"    print(f\"Error during performance testing: {e}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Monitoring and Cleanup"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Monitor endpoint metrics\n",
				"try:\n",
				"    import boto3\n",
				"    \n",
				"    # Get CloudWatch client\n",
				"    cloudwatch = boto3.client('cloudwatch')\n",
				"    \n",
				"    # Get endpoint name\n",
				"    endpoint_name = predictor.endpoint_name\n",
				"    print(f\"Monitoring endpoint: {endpoint_name}\")\n",
				"    \n",
				"    # Get invocation metrics for the last hour\n",
				"    end_time = datetime.datetime.utcnow()\n",
				"    start_time = end_time - datetime.timedelta(hours=1)\n",
				"    \n",
				"    metrics = cloudwatch.get_metric_statistics(\n",
				"        Namespace='AWS/SageMaker',\n",
				"        MetricName='Invocations',\n",
				"        Dimensions=[\n",
				"            {\n",
				"                'Name': 'EndpointName',\n",
				"                'Value': endpoint_name\n",
				"            }\n",
				"        ],\n",
				"        StartTime=start_time,\n",
				"        EndTime=end_time,\n",
				"        Period=300,  # 5-minute periods\n",
				"        Statistics=['Sum']\n",
				"    )\n",
				"    \n",
				"    if metrics['Datapoints']:\n",
				"        total_invocations = sum(dp['Sum'] for dp in metrics['Datapoints'])\n",
				"        print(f\"Total invocations in the last hour: {total_invocations}\")\n",
				"    else:\n",
				"        print(\"No invocation data available for the last hour\")\n",
				"    \n",
				"except Exception as e:\n",
				"    print(f\"Error monitoring endpoint: {e}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# List all deployed endpoints (optional)\n",
				"try:\n",
				"    import boto3\n",
				"    \n",
				"    sagemaker_client = boto3.client('sagemaker')\n",
				"    \n",
				"    # List endpoints\n",
				"    response = sagemaker_client.list_endpoints()\n",
				"    \n",
				"    print(\"Deployed SageMaker endpoints:\")\n",
				"    for endpoint in response['Endpoints']:\n",
				"        print(f\"- {endpoint['EndpointName']} (Status: {endpoint['EndpointStatus']})\")\n",
				"        \n",
				"except Exception as e:\n",
				"    print(f\"Error listing endpoints: {e}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Cleanup function\n",
				"def cleanup_endpoint(endpoint_name=None):\n",
				"    \"\"\"Delete SageMaker endpoint\"\"\"\n",
				"    try:\n",
				"        if endpoint_name is None:\n",
				"            endpoint_name = predictor.endpoint_name\n",
				"        \n",
				"        print(f\"Deleting endpoint: {endpoint_name}\")\n",
				"        sagemaker_client = boto3.client('sagemaker')\n",
				"        \n",
				"        # Delete endpoint\n",
				"        sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
				"        print(f\"Endpoint {endpoint_name} deletion initiated.\")\n",
				"        \n",
				"        # Also delete endpoint config\n",
				"        try:\n",
				"            endpoint_config_name = f\"{endpoint_name}-config\"\n",
				"            sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
				"            print(f\"Endpoint config {endpoint_config_name} deleted.\")\n",
				"        except Exception as e:\n",
				"            print(f\"Note: Could not delete endpoint config: {e}\")\n",
				"        \n",
				"        return True\n",
				"        \n",
				"    except Exception as e:\n",
				"        print(f\"Error during cleanup: {e}\")\n",
				"        return False\n",
				"\n",
				"# Uncomment the line below if you want to delete the endpoint\n",
				"# cleanup_endpoint()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Summary\n",
				"\n",
				"This notebook demonstrates:\n",
				"\n",
				"1. **Setup**: Installing and configuring necessary libraries for SageMaker and Hugging Face\n",
				"2. **Model Loading**: Loading and testing the model locally first\n",
				"3. **Packaging**: Creating a proper inference script and packaging the model\n",
				"4. **Deployment**: Deploying the model to SageMaker with proper configuration\n",
				"5. **Testing**: Comprehensive inference testing and performance monitoring\n",
				"6. **Cleanup**: Proper cleanup of resources\n",
				"\n",
				"### Important Notes:\n",
				"\n",
				"- Make sure you have proper AWS credentials configured\n",
				"- The notebook should be run in a SageMaker instance or with proper IAM permissions\n",
				"- Adjust the instance type and other parameters based on your specific model requirements\n",
				"- Consider using smaller instances for testing and larger ones for production\n",
				"- Remember to clean up resources to avoid unnecessary costs\n",
				"\n",
				"### Customization:\n",
				"\n",
				"- Replace `MODEL_ID` with your specific Sampl model name\n",
				"- Modify the `predict_fn` function based on your model's specific inference requirements\n",
				"- Adjust instance types and configurations as needed\n",
				"- Add additional preprocessing or postprocessing steps as required"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.8.10"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}