{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart Deployment & LangGraph Chat\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Deploy the `deepseek-llm-r1-distill-qwen-1-5b` model using SageMaker JumpStart.\n",
    "2. Authenticate using a specific AWS Profile.\n",
    "3. Integrate the deployed endpoint with LangGraph for stateful chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sagemaker==2.251.1 in /Users/kartik/Documents/smc-repo/chatbot-llm-backend/.conda/lib/python3.12/site-packages (2.251.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "# Installing sagemaker with --no-deps to avoid conflicts on some environments (like Python 3.13 + numpy)\n",
    "%pip install -r requirements.txt -q\n",
    "%pip install 'sagemaker==2.251.1' --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Authentication\n",
    "We configure the AWS session using the specified profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/kartik/Library/Application Support/sagemaker/config.yaml\n",
      "Authenticated with profile: STX-APPLICATION-PLATFORM-ADMIN in region: ap-south-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "# Configuration\n",
    "PROFILE_NAME = 'default'\n",
    "MODEL_ID = \"deepseek-llm-r1-distill-qwen-1-5b\"\n",
    "MODEL_VERSION = \"*\"\n",
    "\n",
    "# Establish Session\n",
    "try:\n",
    "    boto_session = boto3.Session(profile_name=PROFILE_NAME)\n",
    "    sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "    region = boto_session.region_name\n",
    "    print(f\"Authenticated with profile: {PROFILE_NAME} in region: {region}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to use profile {PROFILE_NAME}. Falling back to default credentials.\")\n",
    "    boto_session = boto3.Session()\n",
    "    sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "    region = boto_session.region_name\n",
    "    print(f\"Authenticated with default credentials in region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deploy JumpStart Model\n",
    "We define the JumpStart model and deploy it. \n",
    "**Note**: You must accept the EULA if this is a gated model (`accept_eula=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'deepseek-llm-r1-distill-qwen-1-5b' with wildcard version identifier '*'. You can pin to version '2.20.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment failed or endpoint already exists: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data at s3://jumpstart-cache-prod-ap-south-1/deepseek-llm/deepseek-llm-r1-distill-qwen-1-5b/artifacts/inference-prepack/v2.0.0/. Please ensure that the role \"arn:aws:iam::842675988009:role/aws-reserved/sso.amazonaws.com/ap-south-1/AWSReservedSSO_stx-devops-super-admin-kt4t_75caa9a1ad342c88\" exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\". Also ensure that the role has \"s3:GetObject\" permissions and that the object is located in ap-south-1. If your Model uses multiple models or uncompressed models, please ensure that the role has \"s3:ListBucket\" permission.\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "# We pass the sagemaker_session to ensure it uses the correct profile\n",
    "model = JumpStartModel(\n",
    "    model_id=MODEL_ID,\n",
    "    model_version=MODEL_VERSION,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Deploy the endpoint\n",
    "# accept_eula=True is often required for JumpStart models\n",
    "try:\n",
    "    predictor = model.deploy(accept_eula=True)\n",
    "    ENDPOINT_NAME = predictor.endpoint_name\n",
    "    print(f\"Model deployed successfully! Endpoint Name: {ENDPOINT_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Deployment failed or endpoint already exists: {e}\")\n",
    "    # Attempt to retrieve existing endpoint if deployment fails (e.g. if you ran this cell twice)\n",
    "    # Note: Logic to find existing endpoint by name would go here if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Invoke Test\n",
    "Test the endpoint with a simple payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is Amazon SageMaker?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "}\n",
    "\n",
    "if 'predictor' in locals():\n",
    "    response = predictor.predict(payload)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LangGraph Integration\n",
    "Now we wrap the endpoint in a LangGraph `call_model` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict, List, Dict, Any\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import json\n",
    "\n",
    "# Define Graph State\n",
    "class State(TypedDict):\n",
    "    messages: List[Dict[str, str]]\n",
    "\n",
    "def call_model(state: State):\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Prepare payload\n",
    "    # Using standard Chat API format if supported, or formatting manually.\n",
    "    # Here assuming the model supports 'inputs' string prompts or chat-formatted inputs.\n",
    "    \n",
    "    # Manual formatting for DeepSeek/Llama-3 style\n",
    "    prompt = \"<|begin_of_text|>\"\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"]\n",
    "        prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
    "    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"stop\": [\"<|eot_id|>\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = predictor.predict(payload)\n",
    "        \n",
    "        # Parse Logic\n",
    "        if isinstance(response, bytes):\n",
    "            response_data = json.loads(response.decode('utf-8'))\n",
    "        else:\n",
    "            response_data = response\n",
    "            \n",
    "        content = None\n",
    "        \n",
    "        # Handle [{'generated_text': '...'}]\n",
    "        if isinstance(response_data, list) and len(response_data) > 0:\n",
    "            item = response_data[0]\n",
    "            full_text = item.get('generated_text')\n",
    "            if full_text:\n",
    "                # Strip prompt if echoed\n",
    "                if full_text.startswith(prompt):\n",
    "                    content = full_text[len(prompt):].strip()\n",
    "                else:\n",
    "                    content = full_text.strip()\n",
    "        elif isinstance(response_data, dict) and 'generated_text' in response_data:\n",
    "             content = response_data['generated_text']\n",
    "\n",
    "        if not content:\n",
    "            content = \"Error: No content generated.\"\n",
    "            \n",
    "        return {\"messages\": messages + [{\"role\": \"assistant\", \"content\": content}]}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"messages\": messages + [{\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"}]}\n",
    "\n",
    "# Build Graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_edge(\"agent\", END)\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Chat\n",
    "def chat():\n",
    "    print(\"Starting Chat (type 'quit' to exit)...\")\n",
    "    history = []\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in ['quit', 'exit']:\n",
    "            break\n",
    "        \n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        output = app.invoke({\"messages\": history})\n",
    "        history = output[\"messages\"]\n",
    "        print(f\"Assistant: {history[-1]['content']}\")\n",
    "\n",
    "# chat() # Uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up (Uncomment to delete endpoint when done)\n",
    "# predictor.delete_predictor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}