{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\ude80 Deploy Hugging Face Model to AWS SageMaker\n",
    "\n",
    "**Model**: `openai/gpt-oss-20b` (20B parameters - Cost Optimized)  \n",
    "**AWS Profile**: `default`\n",
    "\n",
    "This notebook provides a step-by-step guide to deploy a Hugging Face model to AWS SageMaker.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udccb Table of Contents\n",
    "\n",
    "1. [Prerequisites & Setup](#1-prerequisites--setup)\n",
    "2. [AWS Configuration](#2-aws-configuration)\n",
    "3. [Model Configuration](#3-model-configuration)\n",
    "4. [Download & Prepare Model](#4-download--prepare-model)\n",
    "5. [Create Inference Script](#5-create-inference-script)\n",
    "6. [Package & Upload to S3](#6-package--upload-to-s3)\n",
    "7. [Deploy to SageMaker](#7-deploy-to-sagemaker)\n",
    "8. [Verify Deployment](#8-verify-deployment)\n",
    "9. [Save Endpoint Configuration](#9-save-endpoint-configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Prerequisites & Setup\n",
    "\n",
    "First, we'll install all required packages and import necessary libraries.\n",
    "\n",
    "> **Note**: This cell may take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install \"sagemaker==2.232.0\" transformers torch accelerate boto3 huggingface_hub bitsandbytes\n",
    "\n",
    "print(\"\\n\u2705 Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2: Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import tarfile\n",
    "import tempfile\n",
    "import traceback\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from huggingface_hub import snapshot_download, login\n",
    "\n",
    "print(f\"\u2705 Libraries imported successfully!\")\n",
    "print(f\"\ud83d\udce6 SageMaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. AWS Configuration\n",
    "\n",
    "Configure AWS credentials using your specified profile: `default`\n",
    "\n",
    "### What this step does:\n",
    "1. Sets up AWS session with your profile\n",
    "2. Retrieves/creates SageMaker execution role\n",
    "3. Configures S3 bucket for model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1: Configure AWS Profile\n",
    "AWS_PROFILE = \"default\"\n",
    "\n",
    "# Set environment variable for AWS profile\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "os.environ['AWS_DEFAULT_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "print(f\"\ud83d\udd10 Using AWS Profile: {AWS_PROFILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.2: Verify AWS credentials\n",
    "try:\n",
    "    # Create session with profile\n",
    "    boto_session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "    sts_client = boto_session.client('sts')\n",
    "    \n",
    "    # Get caller identity to verify credentials\n",
    "    identity = sts_client.get_caller_identity()\n",
    "    \n",
    "    print(\"\u2705 AWS credentials verified successfully!\")\n",
    "    print(f\"   \ud83d\udc64 Account ID: {identity['Account']}\")\n",
    "    print(f\"   \ud83c\udd94 User ARN: {identity['Arn']}\")\n",
    "    print(f\"   \ud83d\udd11 User ID: {identity['UserId']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error verifying AWS credentials: {e}\")\n",
    "    print(\"\\n\ud83d\udca1 Please ensure:\")\n",
    "    print(\"   1. AWS CLI is configured with the profile 'default'\")\n",
    "    print(\"   2. Your credentials are valid and not expired\")\n",
    "    print(\"   3. Run: aws configure --profile default\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.3: Initialize SageMaker session and get execution role\n",
    "\n",
    "# Create SageMaker session with the boto session\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"\ud83c\udf0d AWS Region: {region}\")\n",
    "\n",
    "def get_sagemaker_role(boto_session, account_id):\n",
    "    \"\"\"\n",
    "    Get or find a SageMaker execution role from IAM.\n",
    "    Works in both local notebooks and SageMaker notebooks.\n",
    "    \"\"\"\n",
    "    # First check environment variable\n",
    "    role = os.environ.get('SAGEMAKER_ROLE')\n",
    "    if role:\n",
    "        print(f\"\u2705 Using role from environment: {role}\")\n",
    "        return role\n",
    "    \n",
    "    # Try to find SageMaker roles in IAM\n",
    "    iam_client = boto_session.client('iam')\n",
    "    \n",
    "    print(\"\ud83d\udd0d Searching for SageMaker execution roles...\")\n",
    "    paginator = iam_client.get_paginator('list_roles')\n",
    "    sagemaker_roles = []\n",
    "    \n",
    "    for page in paginator.paginate():\n",
    "        for r in page['Roles']:\n",
    "            role_name = r['RoleName'].lower()\n",
    "            if 'sagemaker' in role_name or 'sage-maker' in role_name:\n",
    "                sagemaker_roles.append(r['Arn'])\n",
    "    \n",
    "    if sagemaker_roles:\n",
    "        print(f\"\\n\ud83d\udccb Found {len(sagemaker_roles)} SageMaker role(s):\")\n",
    "        for i, r in enumerate(sagemaker_roles, 1):\n",
    "            print(f\"   {i}. {r}\")\n",
    "        role = sagemaker_roles[0]  # Use the first one\n",
    "        print(f\"\\n\u2705 Using role: {role}\")\n",
    "        return role\n",
    "    else:\n",
    "        # Create a default role ARN pattern\n",
    "        role = f\"arn:aws:iam::{account_id}:role/service-role/AmazonSageMaker-ExecutionRole\"\n",
    "        print(f\"\\n\u26a0\ufe0f No SageMaker roles found. Using default pattern: {role}\")\n",
    "        print(\"   Please verify this role exists in your AWS account.\")\n",
    "        return role\n",
    "\n",
    "# Get the role\n",
    "role = get_sagemaker_role(boto_session, identity['Account'])\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Final execution role: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.4: Configure S3 bucket\n",
    "\n",
    "# Use default SageMaker bucket or create custom one\n",
    "try:\n",
    "    bucket = sagemaker_session.default_bucket()\n",
    "    print(f\"\u2705 Using S3 bucket: {bucket}\")\n",
    "except Exception as e:\n",
    "    # Create a custom bucket name\n",
    "    account_id = identity['Account']\n",
    "    bucket = f\"sagemaker-{region}-{account_id}\"\n",
    "    print(f\"\ud83d\udce6 Using S3 bucket: {bucket}\")\n",
    "\n",
    "# Verify bucket exists or create it\n",
    "s3_client = boto_session.client('s3')\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=bucket)\n",
    "    print(f\"\u2705 Bucket verified: s3://{bucket}\")\n",
    "except:\n",
    "    print(f\"\u26a0\ufe0f Bucket doesn't exist. Creating: s3://{bucket}\")\n",
    "    try:\n",
    "        if region == 'us-east-1':\n",
    "            s3_client.create_bucket(Bucket=bucket)\n",
    "        else:\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"\u2705 Bucket created successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error creating bucket: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Model Configuration\n",
    "\n",
    "Configure the Hugging Face model to deploy.\n",
    "\n",
    "### Model Details:\n",
    "- **Model ID**: `openai/gpt-oss-20b`\n",
    "- **Parameters**: 20 Billion\n",
    "- **Type**: Large Language Model (LLM)\n",
    "- **Framework**: PyTorch\n",
    "\n",
    "### \ud83d\udcb0 Cost Optimization\n",
    "Using **ml.g5.12xlarge** (~$7/hour) instead of **ml.p4d.24xlarge** (~$32/hour) saves ~75% on costs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1: Configure model parameters\n",
    "\n",
    "# ============================================\n",
    "# \ud83d\udccc MODEL CONFIGURATION - EDIT HERE\n",
    "# ============================================\n",
    "\n",
    "MODEL_ID = \"openai/gpt-oss-20b\"  # Hugging Face model ID (20B parameters)\n",
    "\n",
    "# Instance configuration - optimized for 20B model\n",
    "INSTANCE_TYPE = \"ml.g5.12xlarge\"  # 4x A10G GPUs, 96GB GPU memory (~$7.09/hour)\n",
    "\n",
    "# ============================================\n",
    "# \ud83d\udcb0 COST COMPARISON (On-Demand pricing, us-east-1)\n",
    "# ============================================\n",
    "#\n",
    "# For 7B models:\n",
    "#   - ml.g5.2xlarge   - 1x A10G, 24GB  (~$1.52/hour)\n",
    "#   - ml.g5.4xlarge   - 1x A10G, 24GB  (~$2.03/hour)\n",
    "#\n",
    "# For 13B-20B models:\n",
    "#   - ml.g5.12xlarge  - 4x A10G, 96GB  (~$7.09/hour) \u2705 SELECTED\n",
    "#   - ml.g5.24xlarge  - 4x A10G, 96GB  (~$10.18/hour)\n",
    "#   - ml.p3.8xlarge   - 4x V100, 64GB  (~$14.69/hour)\n",
    "#\n",
    "# For 30B-70B models:\n",
    "#   - ml.g5.48xlarge  - 8x A10G, 192GB (~$20.36/hour)\n",
    "#   - ml.p3.16xlarge  - 8x V100, 128GB (~$28.15/hour)\n",
    "#\n",
    "# For 70B-120B+ models:\n",
    "#   - ml.p4d.24xlarge - 8x A100, 320GB (~$32.77/hour)\n",
    "#\n",
    "# ============================================\n",
    "\n",
    "# Framework versions\n",
    "TRANSFORMERS_VERSION = \"4.36.0\"\n",
    "PYTORCH_VERSION = \"2.1.0\"\n",
    "PY_VERSION = \"py310\"\n",
    "\n",
    "# Endpoint configuration\n",
    "INITIAL_INSTANCE_COUNT = 1\n",
    "ENDPOINT_NAME_PREFIX = \"openai-gpt-oss-20b\"\n",
    "\n",
    "print(\"\ud83d\udccb Model Configuration:\")\n",
    "print(f\"   \ud83e\udd16 Model ID: {MODEL_ID}\")\n",
    "print(f\"   \ud83d\udcbb Instance Type: {INSTANCE_TYPE}\")\n",
    "print(f\"   \ud83d\udcb0 Estimated Cost: ~$7.09/hour\")\n",
    "print(f\"   \ud83d\udd22 Instance Count: {INITIAL_INSTANCE_COUNT}\")\n",
    "print(f\"   \ud83d\udce6 Transformers: {TRANSFORMERS_VERSION}\")\n",
    "print(f\"   \ud83d\udd25 PyTorch: {PYTORCH_VERSION}\")\n",
    "print(f\"   \ud83d\udc0d Python: {PY_VERSION}\")\n",
    "print()\n",
    "print(\"\ud83d\udca1 Cost Savings: Using g5.12xlarge instead of p4d.24xlarge saves ~75%!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: (Optional) Login to Hugging Face Hub\n",
    "# This is required for gated models or private repositories\n",
    "\n",
    "# Option 1: Set token as environment variable\n",
    "HF_TOKEN = os.environ.get('HUGGINGFACE_TOKEN') or os.environ.get('HF_TOKEN') \n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"\u2705 Logged in to Hugging Face Hub\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No Hugging Face token found.\")\n",
    "    print(\"   If the model requires authentication, set HUGGINGFACE_TOKEN environment variable.\")\n",
    "    print(\"   Or run: huggingface-cli login\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Download & Prepare Model\n",
    "\n",
    "Download the model from Hugging Face Hub and prepare it for deployment.\n",
    "\n",
    "> **\u26a0\ufe0f Note**: For 20B parameter models, this step requires ~50GB disk space and may take 10-20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1: Create working directories\n",
    "\n",
    "# Create temporary directories for model files\n",
    "WORK_DIR = tempfile.mkdtemp(prefix=\"sagemaker_model_\")\n",
    "MODEL_DIR = os.path.join(WORK_DIR, \"model\")\n",
    "CODE_DIR = os.path.join(WORK_DIR, \"code\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(CODE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\ud83d\udcc1 Working directory: {WORK_DIR}\")\n",
    "print(f\"\ud83d\udcc1 Model directory: {MODEL_DIR}\")\n",
    "print(f\"\ud83d\udcc1 Code directory: {CODE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.2: Download model from Hugging Face Hub\n",
    "\n",
    "print(f\"\u2b07\ufe0f Downloading model: {MODEL_ID}\")\n",
    "print(\"   This may take 10-20 minutes for 20B models...\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Download model snapshot\n",
    "    downloaded_path = snapshot_download(\n",
    "        repo_id=MODEL_ID,\n",
    "        local_dir=MODEL_DIR,\n",
    "        local_dir_use_symlinks=False,  # Create actual copies\n",
    "        token=HF_TOKEN if HF_TOKEN else None,\n",
    "        resume_download=True  # Resume if interrupted\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\u2705 Model downloaded successfully!\")\n",
    "    print(f\"   \ud83d\udcc1 Location: {downloaded_path}\")\n",
    "    print(f\"   \u23f1\ufe0f Time: {elapsed_time:.2f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "    \n",
    "    # Show model files\n",
    "    model_files = os.listdir(MODEL_DIR)\n",
    "    print(f\"\\n\ud83d\udcc4 Model files ({len(model_files)}):\")\n",
    "    total_size = 0\n",
    "    for f in model_files[:15]:\n",
    "        file_path = os.path.join(MODEL_DIR, f)\n",
    "        if os.path.isfile(file_path):\n",
    "            size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "            total_size += size\n",
    "            print(f\"   - {f} ({size:.2f} MB)\")\n",
    "    if len(model_files) > 15:\n",
    "        print(f\"   ... and {len(model_files) - 15} more files\")\n",
    "    print(f\"\\n   \ud83d\udcca Total size: {total_size/1024:.2f} GB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error downloading model: {e}\")\n",
    "    print(\"\\n\ud83d\udca1 Troubleshooting:\")\n",
    "    print(\"   1. Check if the model ID is correct\")\n",
    "    print(\"   2. For gated models, ensure you have accepted the license\")\n",
    "    print(\"   3. For private models, ensure your token has access\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Create Inference Script\n",
    "\n",
    "Create the custom inference script that SageMaker will use to serve predictions.\n",
    "\n",
    "### The script includes:\n",
    "- `model_fn()`: Load the model\n",
    "- `input_fn()`: Parse incoming requests\n",
    "- `predict_fn()`: Generate predictions\n",
    "- `output_fn()`: Format the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1: Create the inference script\n",
    "\n",
    "INFERENCE_SCRIPT = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "SageMaker Inference Script for Hugging Face LLM\n",
    "Model: openai/gpt-oss-20b\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import traceback\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# Global variables for model and tokenizer\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the model for inference.\n",
    "    This function is called once when the container starts.\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Path to the model artifacts\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing model and tokenizer\n",
    "    \"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading model from: {model_dir}\")\n",
    "        print(f\"Available files: {os.listdir(model_dir)}\")\n",
    "        \n",
    "        # Check available GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_count = torch.cuda.device_count()\n",
    "            print(f\"Available GPUs: {gpu_count}\")\n",
    "            for i in range(gpu_count):\n",
    "                gpu_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "                print(f\"  GPU {i}: {torch.cuda.get_device_name(i)} - {gpu_mem:.1f} GB\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_dir,\n",
    "            padding_side=\"left\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Set pad token if not set\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
    "        \n",
    "        # Load model with automatic device mapping\n",
    "        print(\"Loading model... (this may take a few minutes)\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir,\n",
    "            torch_dtype=torch.float16,  # Use FP16 for efficiency\n",
    "            device_map=\"auto\",  # Automatically distribute across GPUs\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"\u2705 Model loaded successfully!\")\n",
    "        print(f\"   Model type: {type(model).__name__}\")\n",
    "        \n",
    "        return {\"model\": model, \"tokenizer\": tokenizer}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error loading model: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    Parse input data from the request.\n",
    "    \n",
    "    Args:\n",
    "        request_body: The request payload\n",
    "        request_content_type: The content type of the request\n",
    "    \n",
    "    Returns:\n",
    "        dict: Parsed input data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if request_content_type == \"application/json\":\n",
    "            input_data = json.loads(request_body)\n",
    "            return input_data\n",
    "        else:\n",
    "            # Assume text/plain\n",
    "            return {\"inputs\": request_body.decode(\"utf-8\") if isinstance(request_body, bytes) else request_body}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error parsing input: {e}\")\n",
    "        return {\"inputs\": str(request_body)}\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model_artifacts):\n",
    "    \"\"\"\n",
    "    Generate predictions from the model.\n",
    "    \n",
    "    Args:\n",
    "        input_data: Parsed input from input_fn\n",
    "        model_artifacts: Model and tokenizer from model_fn\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = model_artifacts[\"model\"]\n",
    "        tokenizer = model_artifacts[\"tokenizer\"]\n",
    "        \n",
    "        # Extract inputs and parameters\n",
    "        inputs = input_data.get(\"inputs\", input_data.get(\"prompt\", \"\"))\n",
    "        \n",
    "        # Generation parameters with defaults\n",
    "        max_new_tokens = input_data.get(\"max_new_tokens\", 256)\n",
    "        temperature = input_data.get(\"temperature\", 0.7)\n",
    "        top_p = input_data.get(\"top_p\", 0.9)\n",
    "        top_k = input_data.get(\"top_k\", 50)\n",
    "        do_sample = input_data.get(\"do_sample\", True)\n",
    "        repetition_penalty = input_data.get(\"repetition_penalty\", 1.1)\n",
    "        \n",
    "        # Handle string or list inputs\n",
    "        if isinstance(inputs, str):\n",
    "            inputs = [inputs]\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        encoded = tokenizer(\n",
    "            inputs,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=4096  # Adjust based on model's context length\n",
    "        )\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            encoded = {k: v.cuda() for k, v in encoded.items()}\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **encoded,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                do_sample=do_sample,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode outputs\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            outputs,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"generated_text\": generated_texts,\n",
    "            \"input_tokens\": encoded[\"input_ids\"].shape[1],\n",
    "            \"output_tokens\": outputs.shape[1] - encoded[\"input_ids\"].shape[1]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error during prediction: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    \"\"\"\n",
    "    Format the prediction output.\n",
    "    \n",
    "    Args:\n",
    "        prediction: Prediction from predict_fn\n",
    "        response_content_type: Expected response content type\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if response_content_type == \"application/json\":\n",
    "            return json.dumps(prediction, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            # Return as plain text\n",
    "            if isinstance(prediction.get(\"generated_text\"), list):\n",
    "                return \"\\\\n\\\\n\".join(prediction[\"generated_text\"])\n",
    "            return str(prediction)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error formatting output: {e}\")\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "'''\n",
    "\n",
    "# Save the inference script\n",
    "inference_script_path = os.path.join(CODE_DIR, \"inference.py\")\n",
    "with open(inference_script_path, \"w\") as f:\n",
    "    f.write(INFERENCE_SCRIPT)\n",
    "\n",
    "print(f\"\u2705 Inference script created: {inference_script_path}\")\n",
    "\n",
    "# Also copy to model directory\n",
    "shutil.copy(inference_script_path, os.path.join(MODEL_DIR, \"inference.py\"))\n",
    "print(f\"\u2705 Inference script copied to model directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.2: Create requirements.txt for the inference container\n",
    "\n",
    "REQUIREMENTS = '''transformers>=4.36.0\n",
    "torch>=2.1.0\n",
    "accelerate>=0.25.0\n",
    "bitsandbytes>=0.41.0\n",
    "sentencepiece\n",
    "protobuf\n",
    "'''\n",
    "\n",
    "requirements_path = os.path.join(CODE_DIR, \"requirements.txt\")\n",
    "with open(requirements_path, \"w\") as f:\n",
    "    f.write(REQUIREMENTS)\n",
    "\n",
    "# Also copy to model directory\n",
    "shutil.copy(requirements_path, os.path.join(MODEL_DIR, \"requirements.txt\"))\n",
    "\n",
    "print(f\"\u2705 Requirements file created: {requirements_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Package & Upload to S3\n",
    "\n",
    "Package the model and code into a tar.gz file and upload to S3.\n",
    "\n",
    "> **\u26a0\ufe0f Note**: This step may take 5-10 minutes for 20B models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.1: Create model archive (tar.gz)\n",
    "\n",
    "MODEL_TAR_PATH = os.path.join(WORK_DIR, \"model.tar.gz\")\n",
    "\n",
    "print(\"\ud83d\udce6 Creating model archive...\")\n",
    "print(\"   This may take 5-10 minutes for 20B models...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with tarfile.open(MODEL_TAR_PATH, \"w:gz\") as tar:\n",
    "        # Add all files from model directory\n",
    "        for item in os.listdir(MODEL_DIR):\n",
    "            item_path = os.path.join(MODEL_DIR, item)\n",
    "            tar.add(item_path, arcname=item)\n",
    "            print(f\"   Added: {item}\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    file_size = os.path.getsize(MODEL_TAR_PATH) / (1024**3)  # GB\n",
    "    \n",
    "    print(f\"\\n\u2705 Model archive created!\")\n",
    "    print(f\"   \ud83d\udcc1 Path: {MODEL_TAR_PATH}\")\n",
    "    print(f\"   \ud83d\udcca Size: {file_size:.2f} GB\")\n",
    "    print(f\"   \u23f1\ufe0f Time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error creating archive: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.2: Upload model to S3\n",
    "\n",
    "# S3 path for model artifacts\n",
    "MODEL_S3_PREFIX = f\"sagemaker-models/{MODEL_ID.replace('/', '-')}/{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "MODEL_S3_URI = f\"s3://{bucket}/{MODEL_S3_PREFIX}/model.tar.gz\"\n",
    "\n",
    "print(f\"\ud83d\udce4 Uploading model to S3...\")\n",
    "print(f\"   \ud83c\udfaf Destination: {MODEL_S3_URI}\")\n",
    "print(f\"   This may take 5-10 minutes...\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Use SageMaker session for upload (handles multipart automatically)\n",
    "    uploaded_path = sagemaker_session.upload_data(\n",
    "        path=MODEL_TAR_PATH,\n",
    "        bucket=bucket,\n",
    "        key_prefix=MODEL_S3_PREFIX\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\u2705 Model uploaded successfully!\")\n",
    "    print(f\"   \ud83d\udcc1 S3 URI: {uploaded_path}\")\n",
    "    print(f\"   \u23f1\ufe0f Time: {elapsed_time:.2f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "    \n",
    "    MODEL_S3_URI = uploaded_path\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error uploading to S3: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Deploy to SageMaker\n",
    "\n",
    "Create the SageMaker model and deploy it to an endpoint.\n",
    "\n",
    "> **\u23f1\ufe0f Note**: Endpoint creation typically takes 5-10 minutes for 20B models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.1: Create HuggingFace Model object\n",
    "\n",
    "print(\"\ud83c\udfd7\ufe0f Creating SageMaker HuggingFace Model...\")\n",
    "\n",
    "try:\n",
    "    # Create the HuggingFace Model\n",
    "    huggingface_model = HuggingFaceModel(\n",
    "        model_data=MODEL_S3_URI,\n",
    "        role=role,\n",
    "        transformers_version=TRANSFORMERS_VERSION,\n",
    "        pytorch_version=PYTORCH_VERSION,\n",
    "        py_version=PY_VERSION,\n",
    "        entry_point=\"inference.py\",\n",
    "        env={\n",
    "            \"HF_MODEL_ID\": MODEL_ID,\n",
    "            \"HF_TASK\": \"text-generation\",\n",
    "            \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",  # INFO level\n",
    "            \"SAGEMAKER_REGION\": region,\n",
    "        },\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    \n",
    "    print(\"\u2705 HuggingFace Model created successfully!\")\n",
    "    print(f\"   \ud83d\udce6 Model data: {huggingface_model.model_data}\")\n",
    "    print(f\"   \ud83d\udd10 Role: {huggingface_model.role}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error creating HuggingFace model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.2: Deploy the model to SageMaker endpoint\n",
    "\n",
    "# Generate endpoint name with timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "ENDPOINT_NAME = f\"{ENDPOINT_NAME_PREFIX}-{timestamp}\"\n",
    "\n",
    "print(f\"\ud83d\ude80 Deploying model to SageMaker endpoint...\")\n",
    "print(f\"   \ud83d\udcdb Endpoint name: {ENDPOINT_NAME}\")\n",
    "print(f\"   \ud83d\udcbb Instance type: {INSTANCE_TYPE}\")\n",
    "print(f\"   \ud83d\udcb0 Cost: ~$7.09/hour\")\n",
    "print(f\"   \ud83d\udd22 Instance count: {INITIAL_INSTANCE_COUNT}\")\n",
    "print()\n",
    "print(\"   \u23f3 This typically takes 5-10 minutes...\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Deploy the model\n",
    "    predictor = huggingface_model.deploy(\n",
    "        initial_instance_count=INITIAL_INSTANCE_COUNT,\n",
    "        instance_type=INSTANCE_TYPE,\n",
    "        endpoint_name=ENDPOINT_NAME,\n",
    "        container_startup_health_check_timeout=600,  # 10 minutes for model loading\n",
    "        model_data_download_timeout=1200,  # 20 minutes for model download\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\u2705 Model deployed successfully!\")\n",
    "    print(f\"   \ud83d\udcdb Endpoint name: {predictor.endpoint_name}\")\n",
    "    print(f\"   \u23f1\ufe0f Deployment time: {elapsed_time:.2f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error deploying model: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Verify Deployment\n",
    "\n",
    "Test the deployed endpoint with sample requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.1: Check endpoint status\n",
    "\n",
    "sagemaker_client = boto_session.client('sagemaker')\n",
    "\n",
    "try:\n",
    "    response = sagemaker_client.describe_endpoint(EndpointName=ENDPOINT_NAME)\n",
    "    \n",
    "    print(\"\ud83d\udcca Endpoint Status:\")\n",
    "    print(f\"   \ud83d\udcdb Name: {response['EndpointName']}\")\n",
    "    print(f\"   \ud83d\udd04 Status: {response['EndpointStatus']}\")\n",
    "    print(f\"   \ud83d\udcc5 Created: {response['CreationTime']}\")\n",
    "    print(f\"   \ud83d\udd04 Last Modified: {response['LastModifiedTime']}\")\n",
    "    \n",
    "    if response['EndpointStatus'] == 'InService':\n",
    "        print(\"\\n\u2705 Endpoint is ready for inference!\")\n",
    "    else:\n",
    "        print(f\"\\n\u26a0\ufe0f Endpoint is not ready yet. Current status: {response['EndpointStatus']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error checking endpoint status: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.2: Test inference with a sample request\n",
    "\n",
    "print(\"\ud83e\uddea Testing inference...\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a short poem about artificial intelligence.\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"\ud83d\udcdd Prompt: {prompt}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = predictor.predict({\n",
    "            \"inputs\": prompt,\n",
    "            \"max_new_tokens\": 100,\n",
    "            \"temperature\": 0.7,\n",
    "            \"do_sample\": True\n",
    "        })\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\ud83d\udcac Response:\")\n",
    "        if isinstance(response, dict):\n",
    "            if \"generated_text\" in response:\n",
    "                for text in response[\"generated_text\"]:\n",
    "                    print(f\"   {text}\")\n",
    "            else:\n",
    "                print(f\"   {response}\")\n",
    "        else:\n",
    "            print(f\"   {response}\")\n",
    "        \n",
    "        print(f\"\\n\u23f1\ufe0f Response time: {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error during inference: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"\\n\u2705 Inference testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Save Endpoint Configuration\n",
    "\n",
    "Save the endpoint configuration for use in the inference notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9.1: Save endpoint configuration to file\n",
    "\n",
    "endpoint_config = {\n",
    "    \"endpoint_name\": ENDPOINT_NAME,\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"instance_type\": INSTANCE_TYPE,\n",
    "    \"instance_count\": INITIAL_INSTANCE_COUNT,\n",
    "    \"estimated_hourly_cost\": \"$7.09\",\n",
    "    \"region\": region,\n",
    "    \"bucket\": bucket,\n",
    "    \"model_s3_uri\": MODEL_S3_URI,\n",
    "    \"aws_profile\": AWS_PROFILE,\n",
    "    \"created_at\": datetime.datetime.now().isoformat(),\n",
    "    \"transformers_version\": TRANSFORMERS_VERSION,\n",
    "    \"pytorch_version\": PYTORCH_VERSION,\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "config_file = \"endpoint_config.json\"\n",
    "with open(config_file, \"w\") as f:\n",
    "    json.dump(endpoint_config, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\u2705 Endpoint configuration saved to: {config_file}\")\n",
    "print()\n",
    "print(\"\ud83d\udccb Configuration:\")\n",
    "print(json.dumps(endpoint_config, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9.2: Cleanup temporary files\n",
    "\n",
    "print(\"\ud83e\uddf9 Cleaning up temporary files...\")\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(WORK_DIR, ignore_errors=True)\n",
    "    print(f\"   \u2705 Removed: {WORK_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"   \u26a0\ufe0f Could not remove temp directory: {e}\")\n",
    "\n",
    "print(\"\\n\u2705 Cleanup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf89 Deployment Complete!\n",
    "\n",
    "### Summary\n",
    "\n",
    "Your model has been successfully deployed to AWS SageMaker!\n",
    "\n",
    "### Endpoint Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83c\udf89 DEPLOYMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"\ud83d\udcdb Endpoint Name: {ENDPOINT_NAME}\")\n",
    "print(f\"\ud83e\udd16 Model: {MODEL_ID}\")\n",
    "print(f\"\ud83d\udcbb Instance Type: {INSTANCE_TYPE}\")\n",
    "print(f\"\ud83d\udcb0 Estimated Cost: ~$7.09/hour\")\n",
    "print(f\"\ud83c\udf0d Region: {region}\")\n",
    "print(f\"\ud83d\udce6 Model S3 URI: {MODEL_S3_URI}\")\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udcb0 COST SAVINGS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"Using ml.g5.12xlarge instead of ml.p4d.24xlarge:\")\n",
    "print(f\"   \u2022 g5.12xlarge: ~$7.09/hour\")\n",
    "print(f\"   \u2022 p4d.24xlarge: ~$32.77/hour\")\n",
    "print(f\"   \u2022 Savings: ~$25.68/hour (~75%)\")\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udccc NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"1. Use the '02_inference_openai_gpt_oss_120b.ipynb' notebook for inference\")\n",
    "print(f\"2. Endpoint name to use: {ENDPOINT_NAME}\")\n",
    "print(\"3. Remember to delete the endpoint when not in use to avoid costs:\")\n",
    "print(f\"   predictor.delete_endpoint()\")\n",
    "print()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \u26a0\ufe0f Cost Warning & Cleanup\n",
    "\n",
    "**Important**: SageMaker endpoints incur costs as long as they are running (~$7.09/hour for this configuration).\n",
    "\n",
    "To delete the endpoint when you're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u26a0\ufe0f UNCOMMENT THE LINE BELOW TO DELETE THE ENDPOINT\n",
    "# Only run this when you're done with the endpoint!\n",
    "\n",
    "# predictor.delete_endpoint()\n",
    "# print(f\"\u2705 Endpoint '{ENDPOINT_NAME}' deleted successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}